{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5146672,"sourceType":"datasetVersion","datasetId":2990194}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:27:57.796823Z","iopub.status.idle":"2026-01-28T13:27:57.797323Z","shell.execute_reply.started":"2026-01-28T13:27:57.797126Z","shell.execute_reply":"2026-01-28T13:27:57.797159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Import the dataset and print the path\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"mirzahasnine/heart-disease-dataset\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:38:16.645759Z","iopub.execute_input":"2026-01-28T13:38:16.646424Z","iopub.status.idle":"2026-01-28T13:38:17.841616Z","shell.execute_reply.started":"2026-01-28T13:38:16.646388Z","shell.execute_reply":"2026-01-28T13:38:17.840010Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/heart-disease-dataset\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#Check whether it was properly imported\nos.listdir(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:38:22.664837Z","iopub.execute_input":"2026-01-28T13:38:22.666048Z","iopub.status.idle":"2026-01-28T13:38:22.674716Z","shell.execute_reply.started":"2026-01-28T13:38:22.666009Z","shell.execute_reply":"2026-01-28T13:38:22.673174Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3766292177.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Check whether it was properly imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"],"ename":"NameError","evalue":"name 'os' is not defined","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# Build the full path to the CSV file\nfile_path = os.path.join(path, \"heart_disease.csv\")\n\n# Read the CSV into a pandas DataFrame\ndf = pd.read_csv(file_path)\n\n# Preview the first 5 rows\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:38:35.869525Z","iopub.execute_input":"2026-01-28T13:38:35.870742Z","iopub.status.idle":"2026-01-28T13:38:35.879520Z","shell.execute_reply.started":"2026-01-28T13:38:35.870705Z","shell.execute_reply":"2026-01-28T13:38:35.877991Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1415193191.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build the full path to the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"heart_disease.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Read the CSV into a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"],"ename":"NameError","evalue":"name 'os' is not defined","output_type":"error"}],"execution_count":5},{"cell_type":"markdown","source":"## Data Cleaning Log\n\n### Standardization\n- Column names normalized (trimmed, lowercase, spaces to underscores).\n- Outcome column renamed to `heart_stroke`.\n\n### Encoding and Types\n- Binary columns normalized to 0/1 (`currentSmoker`, `BPMeds`, `prevalentStroke`, `prevalentHyp`, `diabetes`, `heart_stroke`).\n- Categorical fields standardized:\n  - `gender` → `Male` / `Female`\n  - `education` → lowercase categories\n- Numeric columns coerced to numeric dtypes (`age`, `cigsPerDay`, `totChol`, `sysBP`, `diaBP`, `BMI`, `heartRate`, `glucose`).\n\n### Duplicates\n- Exact duplicate rows removed (removed: 0).\n\n### Missing Values\n- Missingness observed (percent): glucose 9.16, education 2.48, bpmeds 1.25, totchol 1.18, cigsperday 0.68, bmi 0.45, heartrate 0.02.\n- Imputation rules:\n  - `education`: mode\n  - `bpmeds`: mode\n  - numeric (`glucose`, `totchol`, `bmi`, `heartrate`, `cigsperday`): median\n  - integrity rule: `currentsmoker == 0` ⇒ `cigsperday = 0`\n- Post imputation missingness: 0 across all columns.\n\n### Outliers\n- Plausibility range checks applied; flagged: `sysbp` (1), `totchol` (1).\n- Implausible values set to NaN and re imputed (median).\n\n","metadata":{}},{"cell_type":"code","source":"# =========================\n# Standardize column names\n# =========================\ndf = df.copy()\n\ndf.columns = (\n    df.columns\n      .astype(str)\n      .str.strip()\n      .str.lower()\n      .str.replace(\" \", \"_\", regex=False)\n      .str.replace(\"__\", \"_\", regex=False)\n)\n\nprint(\"Columns after standardization:\")\nprint(df.columns.tolist())\n\n# =========================\n#Rename target column to a stable name\n# =========================\n# Unify any variant of the outcome column into: heart_stroke\ntarget_candidates = [c for c in df.columns if c.replace(\"_\", \"\") == \"heartstroke\"]\nif len(target_candidates) == 1:\n    df = df.rename(columns={target_candidates[0]: \"heart_stroke\"})\nelif \"heart_stroke\" not in df.columns:\n    print(\"Target column not found. Check df.columns and update the rename logic.\")\n\nprint(\"Target column:\", \"heart_stroke\" if \"heart_stroke\" in df.columns else \"NOT FOUND\")\n\n\n# =========================\n# Turn the textdata into numericals\n# =========================\nbinary_map = {\n    \"yes\": 1, \"no\": 0,\n    \"y\": 1, \"n\": 0,\n    \"true\": 1, \"false\": 0,\n    \"1\": 1, \"0\": 0\n}\n\ndef normalize_binary(series: pd.Series) -> pd.Series:\n    if series.dtype == \"O\":\n        s = series.astype(str).str.strip().str.lower()\n        out = s.map(binary_map)\n        out[series.isna()] = np.nan\n        return out\n    return series\n\nbinary_cols = [\n    \"currentsmoker\",\n    \"bpmeds\",\n    \"prevalentstroke\",\n    \"prevalenthyp\",\n    \"diabetes\",\n    \"heart_stroke\"\n]\n\nfor col in binary_cols:\n    if col in df.columns:\n        df[col] = normalize_binary(df[col])\n\nprint(\"Binary columns unique values:\")\nfor col in binary_cols:\n    if col in df.columns:\n        print(col, \":\", pd.unique(df[col]))\n\n\n# =========================\n# Clean all the categorical text fields\n# =========================\nif \"gender\" in df.columns:\n    df[\"gender\"] = df[\"gender\"].astype(str).str.strip().str.title()\n    df.loc[~df[\"gender\"].isin([\"Male\", \"Female\"]), \"gender\"] = np.nan\n\nif \"education\" in df.columns:\n    df[\"education\"] = df[\"education\"].astype(str).str.strip().str.lower()\n    df.loc[df[\"education\"].isin([\"nan\", \"none\", \"\"]), \"education\"] = np.nan\n\nprint(\"Gender unique values:\", df[\"gender\"].dropna().unique() if \"gender\" in df.columns else \"N/A\")\nprint(\"Education unique values:\", df[\"education\"].dropna().unique() if \"education\" in df.columns else \"N/A\")\n\n\n# =========================\n# Properly clean the numeric fields and convert missing ad invalid to NaN\n# =========================\nnumeric_cols = [\n    \"age\", \"cigsperday\", \"totchol\", \"sysbp\", \"diabp\",\n    \"bmi\", \"heartrate\", \"glucose\"\n]\n\nfor col in numeric_cols:\n    if col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\nprint(\"\\nNumeric dtypes snapshot:\")\nprint(df[[c for c in numeric_cols if c in df.columns]].dtypes)\n\n\n# =========================\n# Remove exact duplicate rows\n# =========================\ndup_count = df.duplicated().sum()\ndf = df.drop_duplicates()\n\nprint(\"Exact duplicates removed:\", dup_count)\nprint(\"Shape after duplicate removal:\", df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:38:47.495153Z","iopub.execute_input":"2026-01-28T13:38:47.495451Z","iopub.status.idle":"2026-01-28T13:38:47.513351Z","shell.execute_reply.started":"2026-01-28T13:38:47.495427Z","shell.execute_reply":"2026-01-28T13:38:47.512002Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/531571849.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Standardize column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# =========================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m df.columns = (\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"# =========================\n# Missing value handling\n# =========================\ndf = df.copy()\n\n# Education: mode imputation\nif df[\"education\"].isna().any():\n    edu_mode = df[\"education\"].mode(dropna=True).iloc[0]\n    df[\"education\"] = df[\"education\"].fillna(edu_mode)\n\n# BPMeds: mode imputation (binary)\nif df[\"bpmeds\"].isna().any():\n    bp_mode = df[\"bpmeds\"].mode(dropna=True).iloc[0]\n    df[\"bpmeds\"] = df[\"bpmeds\"].fillna(bp_mode)\n\n# Numeric median imputations\nmedian_impute_cols = [\"glucose\", \"totchol\", \"bmi\", \"heartrate\", \"cigsperday\"]\nfor col in median_impute_cols:\n    if col in df.columns and df[col].isna().any():\n        df[col] = df[col].fillna(df[col].median())\n\n# Smoking integrity rule: non-smokers should have 0 cigs/day\nif set([\"currentsmoker\", \"cigsperday\"]).issubset(df.columns):\n    df.loc[df[\"currentsmoker\"] == 0, \"cigsperday\"] = 0\n\n# =========================\n# Check for missing values\n# =========================\nmissing_after = (df.isna().mean() * 100).sort_values(ascending=False)\nmissing_after = missing_after[missing_after > 0]\nmissing_after\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T05:48:46.466246Z","iopub.execute_input":"2026-01-28T05:48:46.466484Z","iopub.status.idle":"2026-01-28T05:48:46.475485Z","shell.execute_reply.started":"2026-01-28T05:48:46.466465Z","shell.execute_reply":"2026-01-28T05:48:46.474824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Check for outliers\n# =========================\nranges = {\n    \"age\": (18, 100),\n    \"sysbp\": (70, 260),\n    \"diabp\": (40, 150),\n    \"bmi\": (10, 70),\n    \"heartrate\": (30, 220),\n    \"glucose\": (40, 400),\n    \"totchol\": (80, 600),\n    \"cigsperday\": (0, 80)\n}\n\noutlier_counts = {}\nfor col, (lo, hi) in ranges.items():\n    if col in df.columns:\n        outlier_counts[col] = int(((df[col] < lo) | (df[col] > hi)).sum())\n\noutlier_counts\n\n# =========================\n# Outlier handling. Values not making sense only\n# =========================\nranges = {\n    \"age\": (18, 100),\n    \"sysbp\": (70, 260),\n    \"diabp\": (40, 150),\n    \"bmi\": (10, 70),\n    \"heartrate\": (30, 220),\n    \"glucose\": (40, 400),\n    \"totchol\": (80, 600),\n    \"cigsperday\": (0, 80)\n}\n\n# Flag and null implausible values\nfor col, (lo, hi) in ranges.items():\n    if col in df.columns:\n        mask = (df[col] < lo) | (df[col] > hi)\n        df.loc[mask, col] = np.nan\n\n# Re-impute only the columns affected by outliers (median)\nfor col in [\"sysbp\", \"totchol\"]:\n    if col in df.columns:\n        df[col] = df[col].fillna(df[col].median())\n\n# Recheck outliers\noutlier_counts = {}\nfor col, (lo, hi) in ranges.items():\n    if col in df.columns:\n        outlier_counts[col] = int(((df[col] < lo) | (df[col] > hi)).sum())\n\noutlier_counts\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T05:51:45.589319Z","iopub.execute_input":"2026-01-28T05:51:45.58959Z","iopub.status.idle":"2026-01-28T05:51:45.606973Z","shell.execute_reply.started":"2026-01-28T05:51:45.589567Z","shell.execute_reply":"2026-01-28T05:51:45.606272Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering\n\n### Derived Features Created\n- `age_bucket2`: age grouped into clinically interpretable bands (`<40`, `40–49`, `50–59`, `60+`).\n- `bp_category`: blood pressure staging proxy from `sysbp` and `diabp` (`normal`, `elevated`, `stage_1_htn`, `stage_2_htn`).\n- `pulse_pressure`: `sysbp - diabp` (arterial stiffness proxy).\n- `map`: `diabp + (pulse_pressure / 3)` (mean arterial pressure proxy).\n- `bmi_category`: BMI grouped into (`underweight`, `normal`, `overweight`, `obese`).\n- Risk flags:\n  - `high_chol_flag`: `totchol >= 240`\n  - `high_glucose_flag`: `glucose >= 126`\n  - `high_bp_flag`: `sysbp >= 140` or `diabp >= 90`\n- `risk_score_simple`: composite count of selected cardiometabolic risk indicators.\n- Smoking features:\n  - `smoker_flag`: mirrors `currentsmoker`\n  - `heavy_smoker_flag`: `currentsmoker == 1` and `cigsperday >= 20`\n\n### Integrity Checks\n- Validated `sysbp >= diabp` for derived BP features; 1 invalid record was flagged and excluded from pulse pressure/MAP derivation.\n\n### Early Signal Summary (Outcome: `heart_stroke`)\n- Strong monotonic increase in outcome rate across:\n  - `age_bucket2`: <40 (0.041) → 60+ (0.277)\n  - `bp_category`: normal (0.086) → stage_2_htn (0.255)\n  - `risk_score_simple`: score 0 (0.081) → score 5 (0.684)\n- Higher central tendency in engineered BP metrics for cases:\n  - Median `pulse_pressure`: 53 (stroke) vs 46 (no stroke)\n  - Median `map`: 103.67 (stroke) vs 96.67 (no stroke)\n\n","metadata":{}},{"cell_type":"code","source":"# =========================\n#  Feature engineering\n# =========================\n\n# Age buckets (clinically interpretable bands)\ndf[\"age_bucket\"] = pd.cut(\n    df[\"age\"],\n    bins=[0, 39, 49, 59, 69, 120],\n    labels=[\"<40\", \"40-49\", \"50-59\", \"60-69\", \"70+\"],\n    right=True\n)\n\n# Blood pressure category (hypertension staging proxy from sysbp/diabp)\ndef bp_category(row):\n    sbp, dbp = row[\"sysbp\"], row[\"diabp\"]\n    if pd.isna(sbp) or pd.isna(dbp):\n        return np.nan\n    if (sbp < 120) and (dbp < 80):\n        return \"normal\"\n    if (120 <= sbp < 130) and (dbp < 80):\n        return \"elevated\"\n    if (130 <= sbp < 140) or (80 <= dbp < 90):\n        return \"stage_1_htn\"\n    if (sbp >= 140) or (dbp >= 90):\n        return \"stage_2_htn\"\n    return np.nan\n\ndf[\"bp_category\"] = df.apply(bp_category, axis=1)\n\n# Pulse pressure (arterial stiffness proxy)\ndf[\"pulse_pressure\"] = df[\"sysbp\"] - df[\"diabp\"]\n\n# Mean arterial pressure (MAP) proxy\ndf[\"map\"] = df[\"diabp\"] + (df[\"pulse_pressure\"] / 3)\n\n# BMI category (risk grouping proxy)\ndf[\"bmi_category\"] = pd.cut(\n    df[\"bmi\"],\n    bins=[0, 18.5, 25, 30, 100],\n    labels=[\"underweight\", \"normal\", \"overweight\", \"obese\"],\n    right=False\n)\n\n# High cholesterol flag (simple risk flag)\ndf[\"high_chol_flag\"] = (df[\"totchol\"] >= 240).astype(int)\n\n# High glucose flag (proxy for hyperglycemia; threshold commonly used in screening contexts)\ndf[\"high_glucose_flag\"] = (df[\"glucose\"] >= 126).astype(int)\n\n# High BP flag (proxy using sys/dia thresholds)\ndf[\"high_bp_flag\"] = ((df[\"sysbp\"] >= 140) | (df[\"diabp\"] >= 90)).astype(int)\n\n# Composite cardiometabolic risk flag (count style)\nrisk_components = [\"prevalenthyp\", \"diabetes\", \"high_chol_flag\", \"high_glucose_flag\", \"high_bp_flag\"]\ndf[\"risk_score_simple\"] = df[risk_components].sum(axis=1)\n\n# Smoking intensity features\ndf[\"smoker_flag\"] = df[\"currentsmoker\"].astype(int)\ndf[\"heavy_smoker_flag\"] = ((df[\"currentsmoker\"] == 1) & (df[\"cigsperday\"] >= 20)).astype(int)\n\n# Quick verification of new feature columns\nnew_features = [\n    \"age_bucket\", \"bp_category\", \"pulse_pressure\", \"map\", \"bmi_category\",\n    \"high_chol_flag\", \"high_glucose_flag\", \"high_bp_flag\", \"risk_score_simple\",\n    \"smoker_flag\", \"heavy_smoker_flag\"\n]\nprint(\"Engineered features created:\", [c for c in new_features if c in df.columns])\ndf[new_features].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:10:50.268817Z","iopub.execute_input":"2026-01-28T06:10:50.269137Z","iopub.status.idle":"2026-01-28T06:10:50.32326Z","shell.execute_reply.started":"2026-01-28T06:10:50.269115Z","shell.execute_reply":"2026-01-28T06:10:50.322581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# BP integrity: sysbp should be >= diabp\n# =========================\nbp_bad = df[\"sysbp\"] < df[\"diabp\"]\nprint(\"Rows with sysbp < diabp:\", int(bp_bad.sum()))\n\n# Null derived BP features for invalid BP rows\ndf.loc[bp_bad, [\"pulse_pressure\", \"map\"]] = np.nan\n\n# Recompute derived BP features from valid BP rows\ndf.loc[~bp_bad, \"pulse_pressure\"] = df.loc[~bp_bad, \"sysbp\"] - df.loc[~bp_bad, \"diabp\"]\ndf.loc[~bp_bad, \"map\"] = df.loc[~bp_bad, \"diabp\"] + (df.loc[~bp_bad, \"pulse_pressure\"] / 3)\n\n# Quick recheck\nprint(df[\"pulse_pressure\"].describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:10:55.304053Z","iopub.execute_input":"2026-01-28T06:10:55.304341Z","iopub.status.idle":"2026-01-28T06:10:55.319371Z","shell.execute_reply.started":"2026-01-28T06:10:55.304321Z","shell.execute_reply":"2026-01-28T06:10:55.318535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Engineered feature signal vs target\n# =========================\n\n# Age bucket with merged tail\ndf[\"age_bucket2\"] = pd.cut(\n    df[\"age\"],\n    bins=[0, 39, 49, 59, 120],\n    labels=[\"<40\", \"40-49\", \"50-59\", \"60+\"],\n    right=True\n)\n\n# Stroke rate by engineered categorical features (silences FutureWarning)\nfor c in [\"age_bucket2\", \"bp_category\", \"bmi_category\"]:\n    print(f\"\\nHeart stroke rate by {c}:\")\n    print(df.groupby(c, observed=True)[\"heart_stroke\"].mean().sort_values(ascending=False))\n\n# Stroke rate by composite risk score\nprint(\"\\nHeart stroke rate by risk_score_simple:\")\nprint(df.groupby(\"risk_score_simple\")[\"heart_stroke\"].mean())\n\n# Median comparison for engineered continuous features\nprint(\"\\nMedian pulse_pressure and MAP by heart_stroke:\")\nprint(df.groupby(\"heart_stroke\")[[\"pulse_pressure\", \"map\"]].median())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:13:51.149464Z","iopub.execute_input":"2026-01-28T06:13:51.149731Z","iopub.status.idle":"2026-01-28T06:13:51.165218Z","shell.execute_reply.started":"2026-01-28T06:13:51.149712Z","shell.execute_reply":"2026-01-28T06:13:51.16404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Univariate & Multivariate Analysis (by `heart_stroke`)\n\n### Target distribution\n- `heart_stroke = 0`: 3594\n- `heart_stroke = 1`: 644\n- Prevalence: 0.152\n\n### Univariate summaries\n**Numeric (central tendency and dispersion)**\n- Age: median 49, IQR 14\n- SBP: median 128, IQR 27\n- DBP: median 82, IQR 14.875\n- Total cholesterol: median 234, IQR 56\n- BMI: median 25.4, IQR 4.9575\n- Glucose: median 78, IQR 13\n- Pulse pressure: median 47, IQR 16\n- MAP: median 97.33, IQR 17.5\n\n**Categorical (counts)**\n- Gender: Female 2419, Male 1819\n- Education: uneducated 1825, primaryschool 1253, graduate 687, postgraduate 473\n- Age buckets: 40–49 (1660), 50–59 (1333), 60+ (690), <40 (555)\n- BP category: stage_1_htn (1804), normal (1033), stage_2_htn (991), elevated (410)\n- BMI category: normal (1869), overweight (1773), obese (539), underweight (57)\n\n### Cross tabulations (stroke rate by category)\n- Age (`age_bucket2`): <40 (0.041), 40–49 (0.101), 50–59 (0.197), 60+ (0.277)\n- Blood pressure (`bp_category`): normal (0.086), elevated (0.124), stage_1_htn (0.139), stage_2_htn (0.255)\n- BMI category: normal (0.121), underweight (0.140), overweight (0.171), obese (0.195)\n- Gender: Female (0.124), Male (0.189)\n- High BP flag: 0 (0.106), 1 (0.234)\n- Prevalent hypertension: 0 (0.109), 1 (0.247)\n- Diabetes: 0 (0.146), 1 (0.367)\n- High glucose flag: 0 (0.146), 1 (0.453) *(small subgroup: 86 total)*\n- High cholesterol flag: 0 (0.131), 1 (0.178)\n- Heavy smoker flag: 0 (0.140), 1 (0.182)\n- Smoker flag: 0 (0.145), 1 (0.159)\n\n### Multivariate cross tabulations (stroke rate)\n- `age_bucket2 x bp_category`: stroke rate increases with both age band and BP stage, with the highest rates in `60+` and `stage_2_htn`.\n- `gender x risk_score_simple`: stroke rate increases with risk score for both genders; rates are consistently higher among males across risk score levels.\n","metadata":{}},{"cell_type":"code","source":"# =========================\n# Univariate & multivariate analysis\n# =========================\n\n# -------------------------\n# 1) Target distribution\n# -------------------------\ndf[\"heart_stroke\"].value_counts().sort_index()\n\n\n# -------------------------\n# 2) Univariate: numeric (central tendency + dispersion)\n# -------------------------\nnum_cols = [\n    \"age\",\"cigsperday\",\"totchol\",\"sysbp\",\"diabp\",\"bmi\",\"heartrate\",\"glucose\",\n    \"pulse_pressure\",\"map\"\n]\nnum_cols = [c for c in num_cols if c in df.columns]\n\nnum_summary = df[num_cols].describe().T\nnum_summary[\"iqr\"] = num_summary[\"75%\"] - num_summary[\"25%\"]\nnum_summary[[\"count\",\"mean\",\"std\",\"min\",\"25%\",\"50%\",\"75%\",\"max\",\"iqr\"]]\n\n\n# -------------------------\n# 3) Univariate: categorical (frequency tables)\n# -------------------------\ncat_cols = [\n    \"gender\",\"education\",\"age_bucket2\",\"bp_category\",\"bmi_category\",\n    \"smoker_flag\",\"heavy_smoker_flag\",\n    \"high_chol_flag\",\"high_glucose_flag\",\"high_bp_flag\",\n    \"prevalentstroke\",\"prevalenthyp\",\"diabetes\",\"bpmeds\"\n]\ncat_cols = [c for c in cat_cols if c in df.columns]\n\ncat_freq = {c: df[c].value_counts(dropna=False) for c in cat_cols}\ncat_freq\n\n\n# -------------------------\n# 4) Cross-tabulations by target (counts + stroke rate)\n# -------------------------\n# Count cross-tabs\ncrosstabs = {c: pd.crosstab(df[c], df[\"heart_stroke\"], dropna=False) for c in cat_cols}\n\n# Stroke rate by category\nstroke_rates = {c: df.groupby(c, observed=True)[\"heart_stroke\"].mean().sort_values(ascending=False) for c in cat_cols}\n\ncrosstabs, stroke_rates\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:22:56.348469Z","iopub.execute_input":"2026-01-28T06:22:56.348822Z","iopub.status.idle":"2026-01-28T06:22:56.442009Z","shell.execute_reply.started":"2026-01-28T06:22:56.348802Z","shell.execute_reply":"2026-01-28T06:22:56.441068Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation & Association Analysis\n\n### Correlation matrix (numeric features)\n- Spearman correlation was used for the numeric correlation matrix to capture monotonic relationships and reduce sensitivity to skewness and outliers common in clinical variables.\n- A clear BP-related correlation cluster is present: `sysbp` and `diabp` are strongly positively correlated, and both are strongly correlated with derived BP metrics (`map`, `pulse_pressure`).\n- `map` shows the strongest correlations with blood pressure variables, consistent with its construction from `sysbp` and `diabp`.\n- `age` shows a positive association with blood pressure-related measures, aligning with increasing cardiovascular risk with age.\n- `cigsperday` shows weak correlations with most numeric clinical measures, suggesting limited linear/monotonic association at the aggregate level.\n\n### Association with the target (`heart_stroke`)\n- Visual comparisons indicate higher stroke prevalence among older individuals, supported by the stroke-rate-by-age-band plot and the upward shift in the age boxplot for `heart_stroke = 1`.\n- Categorical risk groupings (age bands, BP staging, BMI categories, and composite risk indicators) display meaningful separation in stroke rates across levels.\n\n### Key relationship visualizations\n- Stroke rate increases monotonically across age bands (`<40` → `60+`).\n- The age distribution for `heart_stroke = 1` is shifted upward relative to `heart_stroke = 0`, indicating age as a strong differentiator between cases and non-cases.\n\n","metadata":{}},{"cell_type":"code","source":"# =========================\n# Correlation & Association (target = heart_stroke)\n# =========================\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Output folder for Kaggle downloads\nOUT_DIR = \"/kaggle/working/plots\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\ndef save_plot(filename: str):\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUT_DIR, filename), dpi=300, bbox_inches=\"tight\")\n    plt.show()\n\n\n# =========================\n# 1) Correlation matrix (numeric features)\n# =========================\nnum_cols = [\n    \"age\",\"cigsperday\",\"totchol\",\"sysbp\",\"diabp\",\"bmi\",\"heartrate\",\"glucose\",\n    \"pulse_pressure\",\"map\"\n]\nnum_cols = [c for c in num_cols if c in df.columns]\n\ncorr = df[num_cols].corr(method=\"spearman\")\n\nplt.figure(figsize=(10, 8))\nplt.imshow(corr.values)\nplt.xticks(range(len(corr.columns)), corr.columns, rotation=45, ha=\"right\")\nplt.yticks(range(len(corr.index)), corr.index)\nplt.colorbar(label=\"Spearman correlation\")\nplt.title(\"Spearman correlation matrix (numeric features)\")\nsave_plot(\"corr_matrix_spearman.png\")\n\ncorr\n\n\n# =========================\n# 2) Point-biserial correlation (numeric vs binary target)\n# =========================\npb_rows = []\ny = df[\"heart_stroke\"].astype(int)\n\nfor c in num_cols:\n    x = df[c]\n    mask = x.notna() & y.notna()\n    r, p = stats.pointbiserialr(y[mask], x[mask].astype(float))\n    pb_rows.append({\"feature\": c, \"point_biserial_r\": r, \"p_value\": p})\n\npb_table = pd.DataFrame(pb_rows).sort_values(\"p_value\")\npb_table\n\n\n# =========================\n# 3) Chi-square tests (categorical vs target)\n# =========================\ncat_cols = [\n    \"gender\",\"education\",\"age_bucket2\",\"bp_category\",\"bmi_category\",\n    \"smoker_flag\",\"heavy_smoker_flag\",\n    \"high_chol_flag\",\"high_glucose_flag\",\"high_bp_flag\",\n    \"prevalentstroke\",\"prevalenthyp\",\"diabetes\",\"bpmeds\"\n]\ncat_cols = [c for c in cat_cols if c in df.columns]\n\nchi_rows = []\nfor c in cat_cols:\n    ct = pd.crosstab(df[c], df[\"heart_stroke\"], dropna=False)\n    chi2, p, dof, expected = stats.chi2_contingency(ct)\n    chi_rows.append({\"feature\": c, \"chi2\": chi2, \"dof\": dof, \"p_value\": p})\n\nchi_table = pd.DataFrame(chi_rows).sort_values(\"p_value\")\nchi_table\n\n\n# =========================\n# 4) Visualizations of key relationships\n# =========================\n\n# 4.1 Numeric vs target: boxplot for top point-biserial feature\ntop_num = pb_table.iloc[0][\"feature\"]\ndf.boxplot(column=top_num, by=\"heart_stroke\")\nplt.title(f\"{top_num} by heart_stroke\")\nplt.suptitle(\"\")\nplt.xlabel(\"heart_stroke (0=no, 1=yes)\")\nplt.ylabel(top_num)\nsave_plot(f\"boxplot_{top_num}_by_target.png\")\n\n# 4.2 Categorical vs target: stroke rate bar chart for top chi-square feature\ntop_cat = chi_table.iloc[0][\"feature\"]\nstroke_rate = df.groupby(top_cat, observed=True)[\"heart_stroke\"].mean().sort_values(ascending=False)\n\nstroke_rate.plot(kind=\"bar\")\nplt.ylabel(\"stroke rate\")\nplt.title(f\"Stroke rate by {top_cat}\")\nsave_plot(f\"stroke_rate_{top_cat}.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:31:33.499138Z","iopub.execute_input":"2026-01-28T13:31:33.499466Z","iopub.status.idle":"2026-01-28T13:31:34.499497Z","shell.execute_reply.started":"2026-01-28T13:31:33.499440Z","shell.execute_reply":"2026-01-28T13:31:34.498264Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_73/339847177.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m\"pulse_pressure\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m ]\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mnum_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_cols\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"spearman\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}],"execution_count":5}]}